<!doctype html><html lang=en><head><title>(Multi)linear Algebra | Gabriel H. Brown</title><meta name=viewport content="width=device-width,initial-scale=1"><meta charset=UTF-8><link rel=stylesheet href=//cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.2/css/all.min.css integrity="sha512-1sCRPdkRXhBV2PBLUdRb4tMg1w2YPf37qatUFeS7zlBy7jJI8Lf4VHwWfZZfpXtYSLy85pkm9GaYVYMfw5BC1A==" crossorigin=anonymous><link rel=stylesheet href=//cdnjs.cloudflare.com/ajax/libs/academicons/1.9.1/css/academicons.min.css integrity="sha512-b1ASx0WHgVFL5ZQhTgiPWX+68KjS38Jk87jg7pe+qC7q9YkEtFq0z7xCglv7qGIs/68d3mAp+StfC8WKC5SSAg==" crossorigin=anonymous><link rel=stylesheet href=https://ghbrown.net/css/palettes/color-splotch.css><link rel=stylesheet href=https://ghbrown.net/css/risotto.css><link rel=stylesheet href=https://ghbrown.net/css/custom.css><script src=https://ghbrown.net/assets/quotes.js></script><script src=https://ghbrown.net/js/serve_quote.js></script><script type=text/javascript src=//code.jquery.com/jquery-latest.min.js></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script></head><body><div class=page><header class=page__header><nav class="page__nav main-nav"><ul><h1 class=page__logo><a href=https://ghbrown.net/ class=page__logo-inner>Gabriel H. Brown</a></h1><li class=main-nav__item><a class=nav-main-item href=https://ghbrown.net/ title>About Me</a></li><li class=main-nav__item><a class=nav-main-item href=https://ghbrown.net/topics/research title>Research</a></li><li class=main-nav__item><a class=nav-main-item href=https://ghbrown.net/topics/publications title>Publications</a></li><li class=main-nav__item><a class=nav-main-item href=https://ghbrown.net/topics/open_source title>Open Source</a></li><li class=main-nav__item><a class=nav-main-item href=https://ghbrown.net/topics/hobbies_and_interests title>Hobbies</a></li><li class=main-nav__item><a class=nav-main-item href=https://ghbrown.net/tags title>Post Topics</a></li></ul></nav></header><section class=page__body><h1 id=multilinear-algebra>(Multi)linear Algebra</h1><p>One of my favorite areas of mathematics.
I am especially fond of numerical (multi)linear algebra.</p><ul style=line-height:2em><li><a href=https://ghbrown.net/posts/affine_projection/>Orthogonal projection onto an affine subspace</a></li><p>Projection onto a vector subspace is a common task in linear algebra courses.
Affine subspaces, sets of the form
$
\{ \mathbf{v} \in \mathbb{R}^m : \exists \mathbf{c} :
\mathbf{v} = \mathbf{Ac} + \mathbf{b} \}
$
for fixed matrix $\mathbf{A}$ and ...</p><li><a href=https://ghbrown.net/posts/cg_residual/>Behavior of conjugate gradient residual</a></li><p>An often stressed property of the conjugate gradient method (CG) for solving linear systems is the monotonic decrease in the A-norm of the error.
When CG is applied in practice the exact solution is unknown and the error cannot be computed or tracked, so ...</p><li><a href=https://ghbrown.net/posts/orthogonal_rank_1/>Finding a rank 1 matrix orthogonal to another matrix</a></li><p>Recently I needed to solve the following problem: given a square matrix $\mathbf{A}$ find another square matrix $\mathbf{B}$ satisfying
$\text{vec}(\mathbf{A})^T \text{vec}(\mathbf{B}) = 0$.
This is the sense in which I use the word orthogonal ...</p><li><a href=https://ghbrown.net/posts/sufficient_nonnegative/>A sufficient condition for non-negative solutions to non-negative linear systems</a></li><p>Here we are concerned with &ldquo;non-negative&rdquo; linear systems, that is, linear systems where $\mathbf{A}, \mathbf{b} \geq \mathbf{0}$ (elementwise).
In particular, we give a sufficient condition for ...</p><li><a href=https://ghbrown.net/posts/dim_lu_pivot_needed/>The space of matrices where LU requires pivoting is almost full-dimensional</a></li><p>The pivoted LU factorization of square matrices is the key routine for the direct solution of linear systems.
All square, invertible matrices have a pivoted LU factorization of the form $\mathbf{PA} = \mathbf{LU}$, ...</p><li><a href=https://ghbrown.net/posts/linear_algebra_pronunciation/>Linear Algebra Pronunciation Guide</a></li><p>Like all fields of mathematics, linear algebra has many prominent figures whose names are non-trivial to pronounce in English (the <em>lingua franca</em> of science and mathematics).</p><li><a href=https://ghbrown.net/posts/symmetric_saddle_points/>Interior eigenvectors of symmetric matrices are saddle points</a></li><p>Eigenpairs of symmetric matrices are intimately related to optimization and critical points, with the eigenvectors being critical points of the Rayleigh quotient.
In optimization settings, the type of critical point (minimum, maximum, ...</p><li><a href=https://ghbrown.net/posts/pow_iter_grad_desc/>Relationship between power iteration and gradient descent</a></li><p>Although the majority of successful algorithms for the symmetric tensor eigenvalue problem use optimization techniques directly, there are a few notable algorithms that do not appear to be based on optimization.
Rather, they more closely ...</p></ul></section><section class=page__aside><div class=aside__about><div id=quote><h1>A fun quote</h1></div><script>newQuote()</script></div><hr><div class=aside__content></div></section><footer class=page__footer><p class=copyright>Copyright <a href=/>Gabriel H. Brown</a>.</p></footer></div></body></html>