<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>(Multi)linear Algebra on Gabriel H. Brown</title><link>https://ghbrown.net/tags/linear-algebra/</link><description>Recent content in (Multi)linear Algebra on Gabriel H. Brown</description><generator>Hugo</generator><language>en-us</language><copyright>Copyright [Gabriel H. Brown](/).</copyright><lastBuildDate>Mon, 29 Jul 2024 02:19:38 -0600</lastBuildDate><atom:link href="https://ghbrown.net/tags/linear-algebra/index.xml" rel="self" type="application/rss+xml"/><item><title>Orthogonal projection onto an affine subspace</title><link>https://ghbrown.net/posts/affine_projection/</link><pubDate>Mon, 29 Jul 2024 02:19:38 -0600</pubDate><guid>https://ghbrown.net/posts/affine_projection/</guid><description>&lt;h1 id="orthogonal-projection-onto-an-affine-subspace">Orthogonal projection onto an affine subspace&lt;/h1>
&lt;hr>
&lt;p>Projection onto a vector subspace is a common task in linear algebra courses.
Affine subspaces, sets of the form
$
\{ \mathbf{v} \in \mathbb{R}^m : \exists \mathbf{c} :
 \mathbf{v} = \mathbf{Ac} &amp;#43; \mathbf{b} \}
$
for fixed matrix $\mathbf{A}$ and vector $\mathbf{b}$,
also have well-defined projections.
However, it&amp;rsquo;s difficult to find a direct proof for the projection of a point onto an affine space:
&lt;a href="https://joeyonng.github.io/joeyonng-notebook/Linear%20Algebra/11_Orthogonal_and_Affine_Projection.html" target="_blank">example wordy proof 1&lt;/a>,
&lt;a href="https://math.stackexchange.com/questions/3989922/orthogonal-projection-onto-affine-subspaces-formula" target="_blank">example wordy proof 2&lt;/a>.&lt;/p></description></item><item><title>Behavior of conjugate gradient residual</title><link>https://ghbrown.net/posts/cg_residual/</link><pubDate>Sat, 15 Jun 2024 02:19:38 -0600</pubDate><guid>https://ghbrown.net/posts/cg_residual/</guid><description>&lt;h1 id="behavior-of-the-conjugate-gradient-residual">Behavior of the conjugate gradient residual&lt;/h1>
&lt;hr>
&lt;p>An often stressed property of the conjugate gradient method (CG) for solving linear systems is the monotonic decrease in the A-norm of the error.
When CG is applied in practice the exact solution is unknown and the error cannot be computed or tracked, so the residual or relative residual is used instead.&lt;/p>
&lt;p>However, the kinds of guarantees available for residuals are slightly weaker and more involved than is often presented in courses, due to complications introduced by different norms and arithmetic precisions.
Per &lt;a href="https://www.scirp.org/html/2644.html" target="_blank">an article by Washizawa&lt;/a> we have&lt;/p></description></item><item><title>Finding a rank 1 matrix orthogonal to another matrix</title><link>https://ghbrown.net/posts/orthogonal_rank_1/</link><pubDate>Sat, 06 Apr 2024 10:21:47 -0500</pubDate><guid>https://ghbrown.net/posts/orthogonal_rank_1/</guid><description>&lt;h1 id="finding-a-rank-1-matrix-orthogonal-to-another-matrix">Finding a rank 1 matrix orthogonal to another matrix&lt;/h1>
&lt;hr>
&lt;p>Recently I needed to solve the following problem: given a square matrix $\mathbf{A}$ find another square matrix $\mathbf{B}$ satisfying
$\text{vec}(\mathbf{A})^T \text{vec}(\mathbf{B}) = 0$.
This is the sense in which I use the word orthogonal here, rather than the columns of $\mathbf{A}$ or $\mathbf{B}$ being orthogonal.
Anyway, it&amp;rsquo;s impossible for the columns of a rank 1 matrix to be orthogonal.&lt;/p>
&lt;p>If you are like me this problem sounds: easy at first glance, then quite difficult, then easy again.
Here is my solution.&lt;/p></description></item><item><title>A sufficient condition for non-negative solutions to non-negative linear systems</title><link>https://ghbrown.net/posts/sufficient_nonnegative/</link><pubDate>Fri, 29 Sep 2023 19:19:38 -0600</pubDate><guid>https://ghbrown.net/posts/sufficient_nonnegative/</guid><description>&lt;h1 id="a-sufficient-condition-for-non-negative-solutions-to-non-negative-linear-systems">A sufficient condition for non-negative solutions to non-negative linear systems&lt;/h1>
&lt;hr>
&lt;p>Here we are concerned with &amp;ldquo;non-negative&amp;rdquo; linear systems, that is, linear systems where $\mathbf{A}, \mathbf{b} \geq \mathbf{0}$ (elementwise).
In particular, we give a sufficient condition for non-negativity of the solution $\mathbf{x}$ to $\mathbf{Ax} = \mathbf{b}$.&lt;/p>
&lt;p>First, we discuss an intuition for the result.
The columns of $\mathbf{A}$ form a linear cone; if a vector $\mathbf{c}$ (a right hand side to the linear system) is known to lie in the cone, then one can find a &amp;ldquo;subcone&amp;rdquo; containing $\mathbf{c}$.
The sufficient condition essentially upper bounds the &amp;ldquo;radius&amp;rdquo; of this subcone.&lt;/p></description></item><item><title>The space of matrices where LU requires pivoting is almost full-dimensional</title><link>https://ghbrown.net/posts/dim_lu_pivot_needed/</link><pubDate>Tue, 06 Jun 2023 19:19:38 -0600</pubDate><guid>https://ghbrown.net/posts/dim_lu_pivot_needed/</guid><description>&lt;h1 id="the-space-of-matrices-requiring-pivoted-lu-factorization-is-almost-full-dimensional">The space of matrices requiring pivoted LU factorization is almost full-dimensional&lt;/h1>
&lt;hr>
&lt;p>The pivoted LU factorization of square matrices is the key routine for the direct solution of linear systems.
All square, invertible matrices have a pivoted LU factorization of the form $\mathbf{PA} = \mathbf{LU}$, where $\mathbf{P}$ is a permutation matrix and $\mathbf{L}, \; \mathbf{U}$ are respectively upper and lower triangular.&lt;/p>
&lt;p>However, not all square, invertible matrices have a factorization of the form $\mathbf{A} = \mathbf{LU}$ (an LU factorization).
For example, consider the invertible matrix&lt;/p></description></item><item><title>Linear Algebra Pronunciation Guide</title><link>https://ghbrown.net/posts/linear_algebra_pronunciation/</link><pubDate>Thu, 17 Nov 2022 18:00:23 -0600</pubDate><guid>https://ghbrown.net/posts/linear_algebra_pronunciation/</guid><description>&lt;h1 id="linear-algebra-pronunciation-guide">Linear Algebra Pronunciation Guide&lt;/h1>
&lt;hr>
&lt;p>Like all fields of mathematics, linear algebra has many prominent figures whose names are non-trivial to pronounce in English (the &lt;em>lingua franca&lt;/em> of science and mathematics).&lt;/p>
&lt;p>This is further complicated by numerical linear algebra libraries which have inconsistent and unintuitive pronunciations.&lt;/p>
&lt;hr>
&lt;h3 id="cholesky">Cholesky&lt;/h3>
&lt;ul>
&lt;li>pronounced: ko - less - key (somewhat throaty ko, like kho)&lt;/li>
&lt;li>often mispronounced: chuh - less - key&lt;/li>
&lt;li>notes: &lt;a href="https://en.wikipedia.org/wiki/Andr%C3%A9-Louis_Cholesky" target="_blank" class="mainlink">this is somewhat muddled by that fact that Choleskly was paternally Polish and maternally French, and grew up in France&lt;/a>, but I side with the Polish pronunciation (see references)&lt;/li>
&lt;/ul>
&lt;h3 id="schur">Schur&lt;/h3>
&lt;ul>
&lt;li>pronounced: shore&lt;/li>
&lt;li>often mispronounced: sherr (rhymes with her)&lt;/li>
&lt;/ul>
&lt;h3 id="krylov">Krylov&lt;/h3>
&lt;ul>
&lt;li>pronounced: cree - lav, cree - luvf&lt;/li>
&lt;li>often mispronounced: cry - lawv&lt;/li>
&lt;/ul>
&lt;h3 id="blas">BLAS&lt;/h3>
&lt;ul>
&lt;li>pronounced: blawz (rhymes with paws), bloss (rhymes with gloss, perhaps slightly less common)&lt;/li>
&lt;li>often mispronounced: blass (rhymes with class)&lt;/li>
&lt;/ul>
&lt;h3 id="lapack">LAPACK&lt;/h3>
&lt;ul>
&lt;li>pronounced: L - A - pack (syllabic rhyme with bell - may - pack)&lt;/li>
&lt;li>often mispronounced: luh - pack&lt;/li>
&lt;/ul>
&lt;h3 id="scalapack">SCALAPACK&lt;/h3>
&lt;ul>
&lt;li>pronounced: skay - luh - pack (syllabic rhyme with may - uh - pack)&lt;/li>
&lt;li>often mispronounced: &lt;em>anything else&lt;/em>&lt;/li>
&lt;/ul>
&lt;h3 id="petsc">PETSC&lt;/h3>
&lt;ul>
&lt;li>pronounced: pet - see&lt;/li>
&lt;li>often mispronounced: &lt;em>anything else&lt;/em>&lt;/li>
&lt;/ul>
&lt;p>References:
&lt;a href="https://news.ycombinator.com/item?id=15285881" target="_blank" class="mainlink">Cholesky&lt;/a>,
&lt;a href="https://youtu.be/QlJ4rH6IGe0?t=115" target="_blank" class="mainlink">BLAS and LAPACK&lt;/a>,
&lt;a href="https://petsc.org/release/" target="_blank" class="mainlink">PETSc&lt;/a>&lt;/p></description></item><item><title>Interior eigenvectors of symmetric matrices are saddle points</title><link>https://ghbrown.net/posts/symmetric_saddle_points/</link><pubDate>Fri, 29 Jul 2022 22:19:38 -0600</pubDate><guid>https://ghbrown.net/posts/symmetric_saddle_points/</guid><description>&lt;h1 id="interior-eigenvectors-of-symmetric-matrices-are-saddle-points">Interior eigenvectors of symmetric matrices are saddle points&lt;/h1>
&lt;hr>
&lt;p>Eigenpairs of symmetric matrices are intimately related to optimization and critical points, with the eigenvectors being critical points of the Rayleigh quotient.
In optimization settings, the type of critical point (minimum, maximum, saddle point) is an important feature in designing and understanding algorithms.&lt;/p>
&lt;p>




&lt;a href="https://ghbrown.net/assets/posts/symmetric_saddle_points/Interior%20eigenvectors%20of%20symmetric%20matrices%20are%20saddle%20points%20-%20G.%20H.%20Brown%20and%20E.%20V.%20Solomonik.pdf" class="mainlink" target="_blank">&lt;span class="themecolor">This write up&lt;/span>&lt;/a> characterizes the nature of all critical points (eigenvectors), with the main result being that all interior eigenvectors are saddle points.&lt;/p></description></item><item><title>Relationship between power iteration and gradient descent</title><link>https://ghbrown.net/posts/pow_iter_grad_desc/</link><pubDate>Sun, 29 May 2022 22:19:38 -0600</pubDate><guid>https://ghbrown.net/posts/pow_iter_grad_desc/</guid><description>&lt;h1 id="the-relationship-between-power-iteration-and-gradient-descent">The relationship between power iteration and gradient descent&lt;/h1>
&lt;hr>
&lt;p>Although the majority of successful algorithms for the symmetric tensor eigenvalue problem use optimization techniques directly, there are a few notable algorithms that do not appear to be based on optimization.
Rather, they more closely resemble power iteration and shifted power iteration.&lt;/p>
&lt;p>In 




&lt;a href="https://ghbrown.net/assets/posts/pow_iter_grad_desc/The%20relationship%20between%20power%20iteration%20and%20gradient%20descent%20-%20G.%20H.%20Brown%20and%20E.%20V.%20Solomonik.pdf" class="mainlink" target="_blank">&lt;span class="themecolor">this write up&lt;/span>&lt;/a> we show that this is a false dichotomy, and that these methods are equivalent to very simple and classical optimization methods.
This not only makes these methods easier to conceptualize, but aids in their characterization by permitting the use of established theory from optimization literature.&lt;/p></description></item></channel></rss>